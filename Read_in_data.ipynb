{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130023b-4b55-4ed1-b309-2fe8927ec0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Paths to the .jsonl files\n",
    "file_paths = ['test.jsonl', 'train.jsonl']\n",
    "\n",
    "# Initialize a list to store the data\n",
    "data = []\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    cleaned_text = text.strip('\\n').replace('\\n\\n', '\\n')\n",
    "    return cleaned_text\n",
    "\n",
    "# Load each line from each file, extract \"chosen\" and \"rejected\" text, and append to the list\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Parse the JSON line into a Python dictionary\n",
    "            json_object = json.loads(line)\n",
    "            \n",
    "            # Check and clean \"chosen\" field, if exists, and mark as chosen\n",
    "            if 'chosen' in json_object:\n",
    "                cleaned_chosen_text = clean_text(json_object['chosen'])\n",
    "                data.append({\"text\": cleaned_chosen_text, \"Chosen\": True})\n",
    "            \n",
    "            # Check and clean \"rejected\" field, if exists, and mark as not chosen\n",
    "            if 'rejected' in json_object:\n",
    "                cleaned_rejected_text = clean_text(json_object['rejected'])\n",
    "                data.append({\"text\": cleaned_rejected_text, \"Chosen\": False})\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"text\", \"Chosen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985a6d3d-be27-436a-9f44-d48a65310cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_text_sections(text):\n",
    "    # Split the text by \"Human:\" and \"Assistant:\" to separate the dialogues\n",
    "    parts = text.split(\"Human:\")\n",
    "    human_text = ''\n",
    "    assistant_text = ''\n",
    "    \n",
    "    for part in parts[1:]:  # Skip the first split as it's before the first \"Human:\"\n",
    "        assistant_split = part.split(\"Assistant:\")\n",
    "        human_text += assistant_split[0].strip() + \" \"  # Add human part and a space for separation\n",
    "        \n",
    "        if len(assistant_split) > 1:  # Check if there is an assistant part\n",
    "            assistant_text += assistant_split[1].strip() + \" \"  # Add assistant part and a space for separation\n",
    "            \n",
    "    return human_text.strip(), assistant_text.strip()\n",
    "\n",
    "# Apply the function to each row and create new columns\n",
    "df[['Human', 'Assistant']] = df.apply(lambda row: pd.Series(aggregate_text_sections(row['text'])), axis=1)\n",
    "\n",
    "# Display the updated DataFrame structure\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b72110-03b8-4d00-9f9c-50e5623e8a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_chosen_rejected_dataframe(df):\n",
    "    # Initialize empty lists to store the chosen and rejected text\n",
    "    chosen_text = []\n",
    "    rejected_text = []\n",
    "\n",
    "    # Initialize a counter for mismatched pairs\n",
    "    mismatch_count = 0\n",
    "\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for i in range(0, len(df), 2):\n",
    "        # Check if the 'human' text matches for the current pair of rows\n",
    "        if df.iloc[i]['Human'] == df.iloc[i+1]['Human']:\n",
    "            # Check if the current row has chosen == True\n",
    "            if df.iloc[i]['Chosen']:\n",
    "                chosen_text.append(df.iloc[i]['Assistant'])\n",
    "                rejected_text.append(df.iloc[i+1]['Assistant'])\n",
    "            else:\n",
    "                chosen_text.append(df.iloc[i+1]['Assistant'])\n",
    "                rejected_text.append(df.iloc[i]['Assistant'])\n",
    "        else:\n",
    "            mismatch_count += 1\n",
    "            print(f\"Mismatch found at index {i//2}:\")\n",
    "            print(\"Human text in chosen row:\", df.iloc[i]['Human'])\n",
    "            print(\"Human text in rejected row:\", df.iloc[i+1]['Human'])\n",
    "            print()\n",
    "\n",
    "    # Create a new DataFrame with the chosen and rejected text\n",
    "    result_df = pd.DataFrame({'Chosen': chosen_text, 'Rejected': rejected_text})\n",
    "\n",
    "    # Calculate the percentage of mismatched pairs\n",
    "    total_pairs = len(df) // 2\n",
    "    mismatch_percentage = (mismatch_count / total_pairs) * 100\n",
    "\n",
    "    print(f\"Mismatched pairs: {mismatch_count} out of {total_pairs} ({mismatch_percentage:.2f}%)\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Example usage\n",
    "# Assume 'df' is your input DataFrame with columns ['text', 'chosen', 'human', 'assistant']\n",
    "AB_df = create_chosen_rejected_dataframe(df)\n",
    "print(AB_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f205d6-6a35-4ea9-a1be-a3f5b0a58982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_text(row):\n",
    "    chosen_text = row['Chosen'].split()\n",
    "    rejected_text = row['Rejected'].split()\n",
    "\n",
    "    # Find the index where the text starts to diverge\n",
    "    diverge_index = 0\n",
    "    while diverge_index < min(len(chosen_text), len(rejected_text)):\n",
    "        if chosen_text[diverge_index] != rejected_text[diverge_index]:\n",
    "            break\n",
    "        diverge_index += 1\n",
    "\n",
    "    c_unique = ' '.join(chosen_text[diverge_index:])\n",
    "    r_unique = ' '.join(rejected_text[diverge_index:])\n",
    "\n",
    "    return pd.Series({'C_Unique': c_unique, 'R_Unique': r_unique})\n",
    "\n",
    "# Assuming 'result_df' is the DataFrame obtained from the previous step\n",
    "AB_df[['C_Unique', 'R_Unique']] =AB_df.apply(find_unique_text, axis=1)\n",
    "\n",
    "print(AB_df.head())\n",
    "# Set the output directory\n",
    "output_dir = \"output\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set the initial output file path\n",
    "output_path = os.path.join(output_dir, \"1.csv\")\n",
    "\n",
    "# Add placeholder columns for 'C_Keywords' and 'R_Keywords'\n",
    "AB_df['C_Keywords'] = '~'\n",
    "AB_df['R_Keywords'] = '~'\n",
    "\n",
    "# Save the initial CSV file with placeholders\n",
    "AB_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(AB_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
